{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Callers\n",
    "\n",
    "A `Caller` associates a `Prompt` with a specific LLM client and call parameters (assumes OpenAI-compatibility through a framework like `aisuite`).\n",
    "This allows every Caller instance to use a different model and/or parameters, and sets expectations for the Caller instance.\n",
    "\n",
    "Whereas `Prompts` validate _inputs_ to the template, `Callers` validate the LLM responses.\n",
    "\n",
    "Since Callers leverage the LLM API directly, they can do things like function-calling / tool use.\n",
    "If a tool-call instruction is detected, the Caller can try to `invoke` that call and return the function result as the response.\n",
    "\n",
    "Additionally, Callers can be used as functions/tools in tool-calling workflows by leveraging Caller.signature() which denotes the inputs the Caller's Prompt requires.\n",
    "Since a Caller has a specific client and model assigned, this effectively allows us to use Callers to route to specific models for specific use cases.\n",
    "Since Callers can behave as functions themselves, we enable complex workflows where Callers can call Callers (ad infinitum ad nauseum).\n",
    "\n",
    "- `ChatCaller` is a simple Caller implementation designed for chat messages without response validation.\n",
    "- `RegexCaller` uses regex for response validation.\n",
    "- `StructuredCaller` is intended for structured responses, and uses Pydantic for response validation.\n",
    "- `ToolCaller` is a configuration for tool-use, and can optionally invoke the tool based on arguments in the LLM's response and return the function results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from typing import cast\n",
    "\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "\n",
    "import aisuite\n",
    "import openai\n",
    "\n",
    "from yaaal.core.caller import ChatCaller, RegexCaller, StructuredCaller, ToolCaller\n",
    "from yaaal.core.prompt import (\n",
    "    JinjaMessageTemplate,\n",
    "    PassthroughMessageTemplate,\n",
    "    Prompt,\n",
    "    StaticMessageTemplate,\n",
    "    StringMessageTemplate,\n",
    ")\n",
    "from yaaal.types.base import JSON\n",
    "from yaaal.types.core import Conversation, Message\n",
    "from yaaal.utilities import basic_log_config, format_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_log_config()\n",
    "logging.getLogger(\"yaaal\").setLevel(logging.DEBUG)\n",
    "logger = logging.getLogger(__name__).setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Callers require a client and a model to call.\n",
    "# `yaaal` is built around OpenAI-compatible APIs primarily provided by `aisuite`\n",
    "client = aisuite.Client(\n",
    "    provider_configs={\n",
    "        \"openai\": {\"api_key\": os.environ[\"YAAAL_OPENAI_API_KEY\"]},\n",
    "        \"anthropic\": {\"api_key\": os.environ[\"YAAAL_ANTHROPIC_API_KEY\"]},\n",
    "        # ...\n",
    "    }\n",
    ")\n",
    "# `aisuite` specifies models in \"provider:model\" format\n",
    "model = \"openai:gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:30,706 - DEBUG    - yaaal.core.caller - model:82 - All API requests for ChatCaller will use model : openai:gpt-4o-mini\n",
      "2025-01-27 21:13:30,706 - DEBUG    - yaaal.core.caller - request_params:101 - All API requests for ChatCaller will use params : {'temperature': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# A `ChatCaller`\n",
    "caller = ChatCaller(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    request_params={\"temperature\": 0.7},\n",
    "    prompt=Prompt(\n",
    "        name=\"chat\",\n",
    "        description=\"A simple chat\",\n",
    "        system_template=StaticMessageTemplate(role=\"system\", template=\"You are a helpful assistant\"),\n",
    "        user_template=PassthroughMessageTemplate(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"messages\": [\n",
       "    {\n",
       "      \"role\": \"system\",\n",
       "      \"content\": \"You are a helpful assistant\",\n",
       "    },\n",
       "    {\n",
       "      \"role\": \"user\",\n",
       "      \"content\": \"Who is Harry Potter?\",\n",
       "    },\n",
       "  ],\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# callers can still render conversations through their prompt\n",
    "caller.prompt.render(user_vars={\"content\": \"Who is Harry Potter?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:35,657 - DEBUG    - yaaal.core.caller - _chat_completions_create:163 - Converting response object to ChatCompletion\n",
      "2025-01-27 21:13:35,659 - DEBUG    - yaaal.core.caller - _handle_response:172 - Response object has message.content\n",
      "2025-01-27 21:13:35,659 - DEBUG    - yaaal.core.caller - _validate_content:216 - Using default (passthrough) validator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter is a fictional character and the protagonist of the\n",
      "\"Harry Potter\" series, which consists of seven fantasy novels written\n",
      "by British author J.K. Rowling. The story follows Harry, a young boy\n",
      "who discovers he is a wizard on his eleventh birthday. He attends\n",
      "Hogwarts School of Witchcraft and Wizardry, where he learns magic,\n",
      "makes friends, and faces various challenges.\n",
      "\n",
      "The series explores\n",
      "themes of friendship, bravery, love, and the battle between good and\n",
      "evil, particularly through Harry's ongoing conflict with the dark\n",
      "wizard Lord Voldemort, who killed Harry's parents and seeks to conquer\n",
      "the wizarding world. The books include \"Harry Potter and the\n",
      "Philosopher's Stone\" (published as \"Harry Potter and the Sorcerer's\n",
      "Stone\" in the U.S.), \"Harry Potter and the Chamber of Secrets,\" \"Harry\n",
      "Potter and the Prisoner of Azkaban,\" \"Harry Potter and the Goblet of\n",
      "Fire,\" \"Harry Potter and the Order of the Phoenix,\" \"Harry Potter and\n",
      "the Half-Blood Prince,\" and \"Harry Potter and the Deathly Hallows.\"\n",
      "The series has gained immense popularity worldwide, leading to\n",
      "successful film adaptations, merchandise, a theme park, and an\n",
      "expanded universe, including stage plays and spin-off works like the\n",
      "\"Fantastic Beasts\" film series. Harry Potter has become a significant\n",
      "cultural phenomenon and has had a lasting impact on literature and\n",
      "popular culture.\n"
     ]
    }
   ],
   "source": [
    "# callers are called as functions to get the response from the LLM\n",
    "response = caller(system_vars=None, user_vars={\"content\": \"Who is Harry Potter?\"})\n",
    "\n",
    "print(textwrap.fill(response, replace_whitespace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"$defs\": {\n",
      "    \"PassthroughModel\": {\n",
      "      \"properties\": {\n",
      "        \"content\": {\n",
      "          \"title\": \"Content\",\n",
      "          \"type\": \"string\",\n",
      "        },\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"content\",\n",
      "      ],\n",
      "      \"title\": \"PassthroughModel\",\n",
      "      \"type\": \"object\",\n",
      "    },\n",
      "  },\n",
      "  \"description\": \"A simple chat\",\n",
      "  \"properties\": {\n",
      "    \"user_vars\": {\n",
      "      \"$ref\": \"#/$defs/PassthroughModel\",\n",
      "    },\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"user_vars\",\n",
      "  ],\n",
      "  \"title\": \"chat\",\n",
      "  \"type\": \"object\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# callers have a `signature` method that uses the prompt signature\n",
    "print(format_json(caller.signature().model_json_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:35,689 - DEBUG    - yaaal.core.caller - model:82 - All API requests for RegexCaller will use model : openai:gpt-4o-mini\n",
      "2025-01-27 21:13:35,690 - DEBUG    - yaaal.core.caller - request_params:101 - All API requests for RegexCaller will use params : {'temperature': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# A `RegexCaller` validates the response with a regex pattern\n",
    "pattern = re.compile(r\"\\b[A-E]\\b(?!.*\\b[A-E]\\b)\")\n",
    "\n",
    "template_str = \"\"\"\n",
    "\"The following are multiple choice questions (with answers) about Star Wars.\n",
    "\n",
    "What is the model designation of an X-Wing?\n",
    "A. T-65B\n",
    "B. BTL-A4\n",
    "C. RZ-1\n",
    "D. A/SF-01\n",
    "Answer: A\n",
    "\n",
    "{{question}}\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "class MCQAQuestion(BaseModel):\n",
    "    question: str = Field(description=\"The multiple choice question\")\n",
    "\n",
    "\n",
    "regex_caller = RegexCaller(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    request_params={\"temperature\": 0.7},\n",
    "    prompt=Prompt(\n",
    "        name=\"chat\",\n",
    "        description=\"Multiple-choice question answering\",\n",
    "        system_template=JinjaMessageTemplate(role=\"system\", template=template_str, template_vars_model=MCQAQuestion),\n",
    "    ),\n",
    "    response_validator=pattern,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:36,211 - DEBUG    - yaaal.core.caller - _chat_completions_create:163 - Converting response object to ChatCompletion\n",
      "2025-01-27 21:13:36,212 - DEBUG    - yaaal.core.caller - _handle_response:172 - Response object has message.content\n",
      "2025-01-27 21:13:36,212 - DEBUG    - yaaal.core.caller - _validate_content:292 - Validating response against regex pattern.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Han Solo is:\n",
    "A. A scoundrel\n",
    "B. A scruffy nerfherder\n",
    "C. A smuggler\n",
    "D. The owner of the Millennium Falcon\n",
    "E. All of the above\n",
    "\"\"\".strip()\n",
    "\n",
    "response = regex_caller(system_vars={\"question\": question}, user_vars=None)\n",
    "\n",
    "if response == \"E\":\n",
    "    print(\"Success! ðŸŽ‰\")\n",
    "# print(textwrap.fill(response, replace_whitespace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:36,234 - DEBUG    - yaaal.core.caller - model:82 - All API requests for StructuredCaller will use model : openai:gpt-4o-mini\n",
      "2025-01-27 21:13:36,235 - DEBUG    - yaaal.core.caller - request_params:101 - All API requests for StructuredCaller will use params : {'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'Person', 'strict': True, 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}, 'favorite_color': {'title': 'Favorite Color', 'type': 'string'}}, 'required': ['name', 'age', 'favorite_color'], 'title': 'Person', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'Person'}}}\n"
     ]
    }
   ],
   "source": [
    "# A `StructuredCaller` validates the response with a Pydantic model, and is good for structure data extraction\n",
    "class Person(BaseModel, extra=\"ignore\"):\n",
    "    name: str\n",
    "    age: int\n",
    "    favorite_color: str\n",
    "\n",
    "\n",
    "# Use an fstring to create a jinja prompt --\n",
    "# The fstring allows us to substitute in the Person schema.\n",
    "# Because we're using fstrings, we have to double the `{}`\n",
    "# so python understands they do not indicate an fstring substitution.\n",
    "template_str = f\"\"\"\n",
    "Identify facts about a person as they introduce themselves.\n",
    "\n",
    "Respond in a format that matches the following schema:\n",
    "\n",
    "<schema>\n",
    "{Person.model_json_schema()}\n",
    "</schema>\n",
    "\n",
    "<introduction>\n",
    "{{{{introduction}}}}\n",
    "</introduction>\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "class PersonIntroduction(BaseModel):\n",
    "    introduction: str\n",
    "\n",
    "\n",
    "structured_caller = StructuredCaller(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    request_params={\"temperature\": 0.7},\n",
    "    prompt=Prompt(\n",
    "        name=\"person details\",\n",
    "        description=\"Identify details about a person\",\n",
    "        system_template=JinjaMessageTemplate(\n",
    "            role=\"system\",\n",
    "            template=template_str,\n",
    "            template_vars_model=PersonIntroduction,\n",
    "        ),\n",
    "    ),\n",
    "    response_validator=Person,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:36,663 - DEBUG    - yaaal.core.caller - _chat_completions_create:163 - Converting response object to ChatCompletion\n",
      "2025-01-27 21:13:36,665 - DEBUG    - yaaal.core.caller - _handle_response:176 - Response object has message.tool_call(s), using first.\n",
      "2025-01-27 21:13:36,665 - DEBUG    - yaaal.core.caller - _validate_tool:456 - Validating tool_call response against response_validator Pydantic model (Person).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Person'>\n",
      "{\n",
      "  \"name\": \"Bob\",\n",
      "  \"age\": 42,\n",
      "  \"favorite_color\": \"blue\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "introduction = \"\"\"\n",
    "Hi, my name is Bob and I'm 42.  I work in a button factory, and my favorite color is blue.\n",
    "\"\"\".strip()\n",
    "\n",
    "response = structured_caller(system_vars={\"introduction\": introduction}, user_vars=None)\n",
    "\n",
    "print(type(response))\n",
    "print(format_json(response.model_dump()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:36,680 - DEBUG    - yaaal.core.caller - model:82 - All API requests for ToolCaller will use model : openai:gpt-4o-mini\n",
      "2025-01-27 21:13:36,683 - DEBUG    - yaaal.core.caller - request_params:101 - All API requests for ToolCaller will use params : {'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'chat', 'strict': True, 'parameters': {'$defs': {'MCQAQuestion': {'properties': {'question': {'description': 'The multiple choice question', 'title': 'Question', 'type': 'string'}}, 'required': ['question'], 'title': 'MCQAQuestion', 'type': 'object', 'additionalProperties': False}}, 'description': 'Multiple-choice question answering', 'properties': {'system_vars': {'$ref': '#/$defs/MCQAQuestion'}}, 'required': ['system_vars'], 'title': 'chat', 'type': 'object', 'additionalProperties': False}, 'description': 'Multiple-choice question answering'}}, {'type': 'function', 'function': {'name': 'person_details', 'strict': True, 'parameters': {'$defs': {'PersonIntroduction': {'properties': {'introduction': {'title': 'Introduction', 'type': 'string'}}, 'required': ['introduction'], 'title': 'PersonIntroduction', 'type': 'object', 'additionalProperties': False}}, 'description': 'Identify details about a person', 'properties': {'system_vars': {'$ref': '#/$defs/PersonIntroduction'}}, 'required': ['system_vars'], 'title': 'person_details', 'type': 'object', 'additionalProperties': False}, 'description': 'Identify details about a person'}}], 'tool_choice': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "# A `ToolCaller` can choose to call tools or respond like a normal LLM.\n",
    "\n",
    "template_str = \"\"\"Use the best tool for the task.\"\"\".strip()\n",
    "\n",
    "tool_caller = ToolCaller(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    request_params={\"temperature\": 0.7},\n",
    "    prompt=Prompt(\n",
    "        name=\"tool use\",\n",
    "        description=\"Determine which tool to use\",\n",
    "        system_template=StaticMessageTemplate(role=\"system\", template=template_str),\n",
    "        user_template=PassthroughMessageTemplate(),\n",
    "    ),\n",
    "    toolbox=[regex_caller, structured_caller],  # we can use other callers as tools!\n",
    "    auto_invoke=True,  # we should actually make the recommended tool call\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"temperature\": 0.7,\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"chat\",\n",
      "        \"strict\": true,\n",
      "        \"parameters\": {\n",
      "          \"$defs\": {\n",
      "            \"MCQAQuestion\": {\n",
      "              \"properties\": {\n",
      "                \"question\": {\n",
      "                  \"description\": \"The multiple choice question\",\n",
      "                  \"title\": \"Question\",\n",
      "                  \"type\": \"string\",\n",
      "                },\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"question\",\n",
      "              ],\n",
      "              \"title\": \"MCQAQuestion\",\n",
      "              \"type\": \"object\",\n",
      "              \"additionalProperties\": false,\n",
      "            },\n",
      "          },\n",
      "          \"description\": \"Multiple-choice question answering\",\n",
      "          \"properties\": {\n",
      "            \"system_vars\": {\n",
      "              \"$ref\": \"#/$defs/MCQAQuestion\",\n",
      "            },\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"system_vars\",\n",
      "          ],\n",
      "          \"title\": \"chat\",\n",
      "          \"type\": \"object\",\n",
      "          \"additionalProperties\": false,\n",
      "        },\n",
      "        \"description\": \"Multiple-choice question answering\",\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"person_details\",\n",
      "        \"strict\": true,\n",
      "        \"parameters\": {\n",
      "          \"$defs\": {\n",
      "            \"PersonIntroduction\": {\n",
      "              \"properties\": {\n",
      "                \"introduction\": {\n",
      "                  \"title\": \"Introduction\",\n",
      "                  \"type\": \"string\",\n",
      "                },\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"introduction\",\n",
      "              ],\n",
      "              \"title\": \"PersonIntroduction\",\n",
      "              \"type\": \"object\",\n",
      "              \"additionalProperties\": false,\n",
      "            },\n",
      "          },\n",
      "          \"description\": \"Identify details about a person\",\n",
      "          \"properties\": {\n",
      "            \"system_vars\": {\n",
      "              \"$ref\": \"#/$defs/PersonIntroduction\",\n",
      "            },\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"system_vars\",\n",
      "          ],\n",
      "          \"title\": \"person_details\",\n",
      "          \"type\": \"object\",\n",
      "          \"additionalProperties\": false,\n",
      "        },\n",
      "        \"description\": \"Identify details about a person\",\n",
      "      },\n",
      "    },\n",
      "  ],\n",
      "  \"tool_choice\": \"auto\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# the tool_caller will automatically add the tools to the request parameters\n",
    "print(format_json(tool_caller.request_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:37,905 - DEBUG    - yaaal.core.caller - _chat_completions_create:163 - Converting response object to ChatCompletion\n",
      "2025-01-27 21:13:37,906 - DEBUG    - yaaal.core.caller - _handle_response:176 - Response object has message.tool_call(s), using first.\n",
      "2025-01-27 21:13:37,907 - DEBUG    - yaaal.core.caller - _validate_tool:627 - Validating tool call against tool signature.\n",
      "2025-01-27 21:13:38,413 - DEBUG    - yaaal.core.caller - _chat_completions_create:163 - Converting response object to ChatCompletion\n",
      "2025-01-27 21:13:38,413 - DEBUG    - yaaal.core.caller - _handle_response:176 - Response object has message.tool_call(s), using first.\n",
      "2025-01-27 21:13:38,414 - DEBUG    - yaaal.core.caller - _validate_tool:456 - Validating tool_call response against response_validator Pydantic model (Person).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'yaaal.types.core.ToolMessage'>\n",
      "{\n",
      "  \"role\": \"tool\",\n",
      "  \"content\": \"{\"name\":\"Bob\",\"age\":42,\"favorite_color\":\"blue\"}\",\n",
      "  \"tool_call_id\": \"call_ZRBT5cJFQAzc3kcOoKFoAsfj\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# this should call the person schema tool\n",
    "\n",
    "introduction = \"\"\"\n",
    "Hi, my name is Bob and I'm 42.  I work in a button factory, and my favorite color is blue.\n",
    "\"\"\".strip()\n",
    "\n",
    "response = tool_caller(\n",
    "    system_vars=None,\n",
    "    user_vars={\"content\": introduction},\n",
    ")\n",
    "\n",
    "print(type(response))\n",
    "print(format_json(response.model_dump() if isinstance(response, BaseModel) else response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:40,257 - DEBUG    - yaaal.core.caller - _chat_completions_create:163 - Converting response object to ChatCompletion\n",
      "2025-01-27 21:13:40,258 - DEBUG    - yaaal.core.caller - _handle_response:176 - Response object has message.tool_call(s), using first.\n",
      "2025-01-27 21:13:40,258 - DEBUG    - yaaal.core.caller - _validate_tool:627 - Validating tool call against tool signature.\n",
      "2025-01-27 21:13:40,668 - DEBUG    - yaaal.core.caller - _chat_completions_create:163 - Converting response object to ChatCompletion\n",
      "2025-01-27 21:13:40,669 - DEBUG    - yaaal.core.caller - _handle_response:172 - Response object has message.content\n",
      "2025-01-27 21:13:40,670 - DEBUG    - yaaal.core.caller - _validate_content:292 - Validating response against regex pattern.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'yaaal.types.core.ToolMessage'>\n",
      "{\n",
      "  \"role\": \"tool\",\n",
      "  \"content\": \"E\",\n",
      "  \"tool_call_id\": \"call_VLvac7G4NLr35XKVoOm2kSFi\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# this should call the Star Wars QA tool\n",
    "question = \"\"\"\n",
    "Han Solo is:\n",
    "A. A scoundrel\n",
    "B. A scruffy nerfherder\n",
    "C. A smuggler\n",
    "D. The owner of the Millennium Falcon\n",
    "E. All of the above\n",
    "\"\"\".strip()\n",
    "\n",
    "response = tool_caller(\n",
    "    system_vars=None,\n",
    "    user_vars={\"content\": question},\n",
    ")\n",
    "\n",
    "print(type(response))\n",
    "print(format_json(response.model_dump() if isinstance(response, BaseModel) else response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:13:41,328 - DEBUG    - yaaal.core.caller - _chat_completions_create:163 - Converting response object to ChatCompletion\n",
      "2025-01-27 21:13:41,330 - DEBUG    - yaaal.core.caller - _handle_response:172 - Response object has message.content\n",
      "2025-01-27 21:13:41,330 - DEBUG    - yaaal.core.caller - _validate_content:216 - Using default (passthrough) validator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\"Hello! How can I assist you today?\"\n"
     ]
    }
   ],
   "source": [
    "# this should just respond without calling a tool\n",
    "response = tool_caller(\n",
    "    system_vars=None,\n",
    "    user_vars={\"content\": \"Hello world!\"},\n",
    ")\n",
    "\n",
    "print(type(response))\n",
    "print(format_json(response.model_dump() if isinstance(response, BaseModel) else response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "keep_output": true,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

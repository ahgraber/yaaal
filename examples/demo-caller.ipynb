{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Callers\n",
    "\n",
    "A `Caller` is the basic structure that wraps all logic required for LLM call-and-response.\n",
    "\n",
    "A `Caller` associates a `Prompt` with a specific LLM client and call parameters (assumes OpenAI-compatibility through a framework like `aisuite`).\n",
    "This allows every Caller instance to use a different model and/or parameters, and sets expectations for the Caller instance.\n",
    "Whereas `Prompts` validate _inputs_ to the template and `Handlers` validate the LLM responses, `Callers` make it all happen.\n",
    "\n",
    "Additionally, `Callers` can be used as functions/tools in tool-calling workflows by leveraging `Caller.signature()` which provides the inputs the `Caller.prompt` requires as a JSON schema.\n",
    "Since a `Caller` has a specific client and model assigned, this effectively allows us to use Callers to route to specific models for specific use cases.\n",
    "Since Callers can behave as functions themselves, we enable complex workflows where Callers can call Callers (ad infinitum ad nauseum).\n",
    "\n",
    "Simple factory functions create Callers where the use case is defined by their handlers:\n",
    "\n",
    "- `ChatCaller`: a simple Caller implementation designed for chat messages without response validation.\n",
    "- `RegexCaller`: uses regex for response validation.\n",
    "- `StructuredCaller`:  is intended for structured responses, and uses Pydantic for response validation.\n",
    "- `ToolCaller`: a configuration for tool-use; can optionally invoke the tool based on arguments in the LLM's response and return the function results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from typing import cast\n",
    "\n",
    "import json_repair\n",
    "from pydantic import BaseModel, Field, ValidationError as PydanticValidationError, create_model\n",
    "\n",
    "import aisuite\n",
    "import openai\n",
    "\n",
    "from yaaal.core.caller import Caller, create_chat_caller, create_structured_caller, create_tool_caller\n",
    "from yaaal.core.handler import CompositeHandler, ResponseHandler, ToolHandler\n",
    "from yaaal.core.prompt import (\n",
    "    JinjaMessageTemplate,\n",
    "    PassthroughMessageTemplate,\n",
    "    Prompt,\n",
    "    StaticMessageTemplate,\n",
    "    StringMessageTemplate,\n",
    ")\n",
    "from yaaal.core.validator import PassthroughValidator, PydanticValidator, RegexValidator, ToolValidator\n",
    "from yaaal.types.base import JSON\n",
    "from yaaal.types.core import Conversation, Message\n",
    "from yaaal.utilities import basic_log_config, format_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_log_config()\n",
    "logging.getLogger(\"yaaal\").setLevel(logging.DEBUG)\n",
    "logger = logging.getLogger(__name__).setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Callers require a client and a model to call.\n",
    "# `yaaal` is built around OpenAI-compatible APIs primarily provided by `aisuite`\n",
    "client = aisuite.Client(\n",
    "    provider_configs={\n",
    "        \"openai\": {\"api_key\": os.environ[\"YAAAL_OPENAI_API_KEY\"]},\n",
    "        \"anthropic\": {\"api_key\": os.environ[\"YAAAL_ANTHROPIC_API_KEY\"]},\n",
    "        # ...\n",
    "    }\n",
    ")\n",
    "# `aisuite` specifies models in \"provider:model\" format\n",
    "model = \"openai:gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `ChatCaller`\n",
    "caller = create_chat_caller(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    request_params={\"temperature\": 0.7},\n",
    "    prompt=Prompt(\n",
    "        name=\"chat\",\n",
    "        description=\"A simple chat\",\n",
    "        system_template=StaticMessageTemplate(role=\"system\", template=\"You are a helpful assistant\"),\n",
    "        user_template=PassthroughMessageTemplate(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callers can still render conversations through their prompt\n",
    "caller.prompt.render(user_vars={\"content\": \"Who is Harry Potter?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callers are called as functions to get the response from the LLM\n",
    "response = caller(system_vars=None, user_vars={\"content\": \"Who is Harry Potter?\"})\n",
    "\n",
    "print(textwrap.fill(response.content, replace_whitespace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callers have a `signature` method that uses the prompt signature\n",
    "print(format_json(caller.signature().model_json_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `RegexCaller` validates the response with a regex pattern\n",
    "pattern = re.compile(r\"\\b[A-E]\\b(?!.*\\b[A-E]\\b)\")\n",
    "\n",
    "template_str = \"\"\"\n",
    "\"The following are multiple choice questions (with answers) about Star Wars.\n",
    "\n",
    "What is the model designation of an X-Wing?\n",
    "A. T-65B\n",
    "B. BTL-A4\n",
    "C. RZ-1\n",
    "D. A/SF-01\n",
    "Answer: A\n",
    "\n",
    "{{question}}\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "class MCQAQuestion(BaseModel):\n",
    "    question: str = Field(description=\"The multiple choice question\")\n",
    "\n",
    "\n",
    "regex_caller = Caller(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    request_params={\"temperature\": 0.7},\n",
    "    prompt=Prompt(\n",
    "        name=\"chat\",\n",
    "        description=\"Multiple-choice question answering\",\n",
    "        system_template=JinjaMessageTemplate(role=\"system\", template=template_str, template_vars_model=MCQAQuestion),\n",
    "    ),\n",
    "    handler=ResponseHandler(validator=RegexValidator(pattern=pattern)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Han Solo is:\n",
    "A. A scoundrel\n",
    "B. A scruffy nerfherder\n",
    "C. A smuggler\n",
    "D. The owner of the Millennium Falcon\n",
    "E. All of the above\n",
    "\"\"\".strip()\n",
    "\n",
    "response = regex_caller(system_vars={\"question\": question}, user_vars=None)\n",
    "\n",
    "if response.content == \"E\":\n",
    "    print(\"Success! ðŸŽ‰\")\n",
    "# print(textwrap.fill(response, replace_whitespace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `StructuredCaller` validates the response with a Pydantic model, and is good for structure data extraction\n",
    "class Person(BaseModel, extra=\"ignore\"):\n",
    "    name: str\n",
    "    age: int\n",
    "    favorite_color: str\n",
    "\n",
    "\n",
    "# Use an fstring to create a jinja prompt --\n",
    "# The fstring allows us to substitute in the Person schema.\n",
    "# Because we're using fstrings, we have to double the `{}`\n",
    "# so python understands they do not indicate an fstring substitution.\n",
    "template_str = f\"\"\"\n",
    "Identify facts about a person as they introduce themselves.\n",
    "\n",
    "Respond in a format that matches the following schema:\n",
    "\n",
    "<schema>\n",
    "{Person.model_json_schema()}\n",
    "</schema>\n",
    "\n",
    "<introduction>\n",
    "{{{{introduction}}}}\n",
    "</introduction>\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "class PersonIntroduction(BaseModel):\n",
    "    introduction: str\n",
    "\n",
    "\n",
    "structured_caller = create_structured_caller(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    request_params={\"temperature\": 0.7},\n",
    "    prompt=Prompt(\n",
    "        name=\"person details\",\n",
    "        description=\"Identify details about a person\",\n",
    "        system_template=JinjaMessageTemplate(\n",
    "            role=\"system\",\n",
    "            template=template_str,\n",
    "            template_vars_model=PersonIntroduction,\n",
    "        ),\n",
    "    ),\n",
    "    response_model=Person,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction = \"\"\"\n",
    "Hi, my name is Bob and I'm 42.  I work in a button factory, and my favorite color is blue.\n",
    "\"\"\".strip()\n",
    "\n",
    "response = structured_caller(system_vars={\"introduction\": introduction}, user_vars=None)\n",
    "\n",
    "print(type(response))\n",
    "print(format_json(response.model_dump()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callers using Pydantic Handlers still return an AssistantMessage; it was validated internally before returning to the user.\n",
    "\n",
    "This means we still have to re-validate if we want the response as a Pydantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Person.model_validate_json(response.content)\n",
    "print(type(p))\n",
    "print(format_json(p.model_dump()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `ToolCaller` can choose to call tools or respond like a normal LLM.\n",
    "\n",
    "template_str = \"\"\"Use the best tool for the task.\"\"\".strip()\n",
    "\n",
    "tool_caller = create_tool_caller(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    request_params={\"temperature\": 0.7},\n",
    "    prompt=Prompt(\n",
    "        name=\"tool use\",\n",
    "        description=\"Determine which tool to use\",\n",
    "        system_template=StaticMessageTemplate(role=\"system\", template=template_str),\n",
    "        user_template=PassthroughMessageTemplate(),\n",
    "    ),\n",
    "    toolbox=[regex_caller, structured_caller],  # we can use other callers as tools!\n",
    "    auto_invoke=True,  # we should actually make the recommended tool call\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tool_caller will automatically add the tools to the request parameters\n",
    "print(format_json(tool_caller.request_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should call the person schema tool\n",
    "\n",
    "introduction = \"\"\"\n",
    "Hi, my name is Bob and I'm 42.  I work in a button factory, and my favorite color is blue.\n",
    "\"\"\".strip()\n",
    "\n",
    "response = tool_caller(\n",
    "    system_vars=None,\n",
    "    user_vars={\"content\": introduction},\n",
    ")\n",
    "\n",
    "# remember we have to convert to Pydantic model\n",
    "# try:\n",
    "p = Person.model_validate_json(response.content)\n",
    "# except PydanticValidationError:\n",
    "#     p = Person.model_validate(json_repair.loads(response.content))\n",
    "\n",
    "print(type(p))\n",
    "print(format_json(p.model_dump()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should call the Star Wars QA tool\n",
    "question = \"\"\"\n",
    "Han Solo is:\n",
    "A. A scoundrel\n",
    "B. A scruffy nerfherder\n",
    "C. A smuggler\n",
    "D. The owner of the Millennium Falcon\n",
    "E. All of the above\n",
    "\"\"\".strip()\n",
    "\n",
    "response = tool_caller(\n",
    "    system_vars=None,\n",
    "    user_vars={\"content\": question},\n",
    ")\n",
    "\n",
    "print(type(response))\n",
    "print(format_json(response.model_dump() if isinstance(response, BaseModel) else response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will error because tool_caller *only* calls tools\n",
    "response = tool_caller(\n",
    "    system_vars=None,\n",
    "    user_vars={\"content\": \"Hello world!\"},\n",
    ")\n",
    "\n",
    "print(type(response))\n",
    "print(format_json(response.model_dump() if isinstance(response, BaseModel) else response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `tool_caller` as created above *only* calls tools.\n",
    "To make a `Caller` that is also able to use standard chat completions, use a `CompositeHandler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_caller = Caller(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    request_params={\"temperature\": 0.7},\n",
    "    prompt=Prompt(\n",
    "        name=\"tool use\",\n",
    "        description=\"Determine which tool to use\",\n",
    "        system_template=StaticMessageTemplate(role=\"system\", template=template_str),\n",
    "        user_template=PassthroughMessageTemplate(),\n",
    "    ),\n",
    "    handler=CompositeHandler(\n",
    "        content_handler=ResponseHandler(PassthroughValidator()),\n",
    "        tool_handler=ToolHandler(\n",
    "            validator=ToolValidator(\n",
    "                toolbox=[regex_caller, structured_caller],  # we can use other callers as tools!\n",
    "            ),\n",
    "            auto_invoke=True,  # we should actually make the recommended tool call\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = all_caller(\n",
    "    system_vars=None,\n",
    "    user_vars={\"content\": \"Hello world!\"},\n",
    ")\n",
    "\n",
    "print(type(response))\n",
    "print(format_json(response.model_dump() if isinstance(response, BaseModel) else response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should call the Star Wars QA tool\n",
    "question = \"\"\"\n",
    "Han Solo is:\n",
    "A. A scoundrel\n",
    "B. A scruffy nerfherder\n",
    "C. A smuggler\n",
    "D. The owner of the Millennium Falcon\n",
    "E. All of the above\n",
    "\"\"\".strip()\n",
    "\n",
    "response = all_caller(\n",
    "    system_vars=None,\n",
    "    user_vars={\"content\": question},\n",
    ")\n",
    "\n",
    "print(type(response))\n",
    "print(format_json(response.model_dump() if isinstance(response, BaseModel) else response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
